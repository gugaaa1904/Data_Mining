{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"top\"></a>  üìä DM2425_ABCDEats\n",
    "Authors:<br><br>\n",
    "Student Name - Gon√ßalo Cust√≥dio<br>\n",
    "- Student id - 20211643<br>\n",
    "- Contact e-mail - 20211643@novaims.unl.pt<br>\n",
    "  \n",
    "Student Name - Diogo Correia<br>\n",
    "- Student id - 20211586<br>\n",
    "- Contact e-mail - 20211586@novaims.unl.pt<br>\n",
    "  \n",
    "Student Name - Jo√£o Santos<br>\n",
    "- Student id - 20211691<br>\n",
    "- Contact e-mail - 20211691@novaims.unl.pt<br>\n",
    "  \n",
    "Student Name - Nuno Bernardino<br>\n",
    "- Student id - 20211546<br>\n",
    "- Contact e-mail - 20211546@novaims.unl.pt<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "1. [Exploration of the Dataset](#1-exploration-of-the-dataset)   \n",
    "   1.2 [Identify Trends, Patterns, or Anomalies](#3-identify-trends-patterns-or-anomalies)  \n",
    "   3.1 [Descriptive Statistics](#3.1-descriptive-statistics)  \n",
    "   3.2 [Visualization](#3.2-visualization)  \n",
    "       3.2.1 [Histograms](#3.2.1-histograms)  \n",
    "       3.2.2 [Boxplots](#3.2.2-boxplots)  \n",
    "       3.2.3 [Categorical Variables](#3.2.3-categorical-variables)  \n",
    "       3.2.4 [Correlation Analysis](#3.2.4-correlation-analysis)  \n",
    "   3.3 [Trend Analysis](#3.3-trend-analysis) \n",
    "\n",
    "\n",
    "2. [Preprocessing](#2-preprocessing)  \n",
    "   2.1 [Numeric and Categorical Variables](#2.1-numeric-and-categorical-variables)  \n",
    "   2.2 [Duplicated Values Treatment](#2.2-duplicated-values-treatment)  \n",
    "   2.3 [Missing Values Treatment](#2.3-missing-values-treatment)  \n",
    "   2.4 [Impute the Missing Values (KNN)](#2.4-impute-the-missing-values-knn)   \n",
    "\n",
    "4. [Outliers](#4-outliers)  \n",
    "   4.1 [Outliers Removal](#4.1-outliers-removal)  \n",
    "   4.2 [Missing Values Treatment after Outlier Removal](#4.2-missing-values-treatment-after-outlier-removal)  \n",
    "   4.3 [Outliers Check After Treatment](#4.3-outliers-check-after-treatment)  \n",
    "\n",
    "5. [Create New Features](#5-create-new-features)  \n",
    "   5.1 [Visualizations of New Features](#5.1-visualizations-of-new-features)  \n",
    "   5.2 [Impute Missing Values of New Features (KNN)](#5.2-impute-missing-values-of-new-features-knn)  \n",
    "\n",
    "6. [Feature Selection](#6-feature-selection)  \n",
    "   6.1 [Correlation with New Features](#6.1-correlation-with-new-features)  \n",
    "   6.2 [Cramer V (Categorical Features)](#6.2-cramer-v-categorical-features)  \n",
    "\n",
    "7. [Variance Evaluation](#7-variance-evaluation)  \n",
    "\n",
    "8. [Scaling Numerical Data (MinMaxScaler)](#8-scaling-numerical-data-minmaxscaler)  \n",
    "\n",
    "9. [Export](#9-export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kmodes\n",
    "#!pip install somoclu\n",
    "#!pip install minisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"DM2425_ABCDEats_DATASET.xlsx\", sheet_name=\"DM2425_ABCDEats_DATASET\")\n",
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the index to the customer_id column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('customer_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploration of the Dataset\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Analysis**\n",
    "\n",
    "To kick off our deep exploration, we‚Äôll use the `data.info()` command to get an overview of the dataset. This command provides essential information, including the number of entries, column names, non-null counts, and data types for each variable. This quick summary will allow us to identify any missing values, spot potential data type issues, and gain a high-level understanding of the dataset's structure, setting the stage for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check data types of our variables**\n",
    "\n",
    "In this step, we‚Äôll use the `data.dtypes` command to examine the data types of each variable in our dataset. This overview will confirm if the variables are appropriately typed (e.g., integers, floats, objects) and will help us spot any inconsistencies or unexpected types that might require adjustment. Understanding the data types at this stage is crucial, as it guides us in selecting suitable preprocessing and analysis techniques for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Stats and Missing Values Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Orders per Hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_columns = [f'HR_{i}' for i in range(24)]\n",
    "hourly_order_volume = data[hour_columns].sum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=hourly_order_volume.index, y=hourly_order_volume.values, color='orange')\n",
    "plt.title(\"Volume of Total Orders per Hour\", fontsize=16)\n",
    "plt.xlabel(\"Hours\", fontsize=12)\n",
    "plt.ylabel(\"Orders\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we don't agree with the fact that the variables in question (last_promo, payment_method and customer_region) are of type object we will change them to category in order to facilitate future analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Numeric and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['last_promo'] = data['last_promo'].astype('category')\n",
    "data['payment_method'] = data['payment_method'].astype('category')\n",
    "data['customer_region'] = data['customer_region'].astype('category')\n",
    "data['customer_age'] = data['customer_age'].fillna(0).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will divide the variables into lists for categorical and numerical variables to facilitate future interactions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_var = ['customer_region', 'last_promo', 'payment_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_var = ['customer_age', 'vendor_count', 'product_count', 'is_chain', 'first_order', 'last_order', \n",
    "              'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe', 'CUI_Chicken Dishes', 'CUI_Chinese', \n",
    "              'CUI_Desserts', 'CUI_Healthy', 'CUI_Indian', 'CUI_Italian', 'CUI_Japanese', 'CUI_Noodle Dishes', \n",
    "              'CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai', 'DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', \n",
    "              'DOW_5', 'DOW_6', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', \n",
    "              'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', \n",
    "              'HR_21', 'HR_22', 'HR_23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical Variable Types:\")\n",
    "print(data[category_var].dtypes)\n",
    "print(\"\\nNumerical Variable Types:\")\n",
    "print(data[number_var].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Duplicated Values Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data[data.index.duplicated(keep=False)]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.index.duplicated(keep='first')]\n",
    "\n",
    "print(f\"New number of lines in the dataset: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Missing Values Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data[number_var] = knn_imputer.fit_transform(data[number_var])\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Identify Trends, Patterns, or Anomalies\n",
    "\n",
    "\n",
    "\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[number_var].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in category_var:\n",
    "    print(f\"Distribution for {var}:\")\n",
    "    print(data[var].value_counts(normalize=True) * 100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[number_var].hist(figsize=(15, 10), bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features = ['customer_region', 'customer_age', 'vendor_count', 'product_count','is_chain', 'first_order', 'last_order', 'last_promo', 'payment_method']\n",
    "\n",
    "cuisine_columns = [col for col in data.columns if col.startswith('CUI_')]\n",
    "dow_columns = [col for col in data.columns if col.startswith('DOW_')]\n",
    "hr_columns = [col for col in data.columns if col.startswith('HR_')]\n",
    "\n",
    "groups = {\n",
    "    'Customer Region & Related Features': other_features,\n",
    "    'Customer Age & Vendor Count': ['customer_age', 'vendor_count'],\n",
    "    'Cuisines': cuisine_columns,\n",
    "    'Days of the Week': dow_columns,\n",
    "    'Hours of the Day': hr_columns\n",
    "}\n",
    "\n",
    "for group_name, columns in groups.items():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    data[columns].boxplot(vert=False, grid=False)\n",
    "    plt.title(f'Boxplots for {group_name}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in category_var:\n",
    "    data[var].value_counts().plot(kind='bar', title=f\"Distribution of {var}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Correlation between numeric features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "corr = data[number_var].corr()\n",
    "sns.heatmap(corr[(corr > 0.8) | (corr < -0.8)], annot=True, cmap='coolwarm', mask=(corr <= 0.8) & (corr >= -0.8))\n",
    "plt.title(\"Heatmap of High Correlations (|correlation| > 0.5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Table Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data[number_var].corr()\n",
    "\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) >= 0.8 or abs(correlation_matrix.iloc[i, j]) < -0.8: \n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Variable 1', 'Variable 2', 'Correlation'])\n",
    "\n",
    "high_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Correlation between categorical features (Cramers V)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data has all these columns\n",
    "assert all(col in data.columns for col in category_var), \"Some columns are missing in data\"\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    contingency_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(contingency_matrix)[0]\n",
    "    n = contingency_matrix.sum().sum()\n",
    "    r, k = contingency_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "correlation_matrix = pd.DataFrame(index=category_var, columns=category_var)\n",
    "\n",
    "for col1 in category_var:\n",
    "    for col2 in category_var:\n",
    "        if col1 == col2:\n",
    "            correlation_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            correlation_matrix.loc[col1, col2] = cramers_v(\n",
    "                data[col1].round(3),\n",
    "                data[col2].round(3)\n",
    "            )\n",
    "\n",
    "correlation_matrix = correlation_matrix.astype(float)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(correlation_matrix,annot=True, fmt=\".2f\",cmap=\"coolwarm\",vmin=0, vmax=1,cbar=True,)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_data = data[[f'DOW_{i}' for i in range(7)]].sum()\n",
    "dow_data.plot(kind='bar', title=\"Orders by Day of the Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[col for col in number_var if 'CUI_' in col]].mean().plot(kind='bar')\n",
    "plt.title(\"Average Spending by Cuisine Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Outliers\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = data[number_var].apply(zscore)\n",
    "outliers = (z_scores.abs() > 3).sum()\n",
    "print(\"Outliers per variable:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Outliers Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_removal(data, number_var):\n",
    "    for col in number_var:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        data[col] = data[col].where((data[col] >= lower_bound) & (data[col] <= upper_bound), np.nan)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_removal(data, number_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Missing Values Treatment after Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[number_var] = knn_imputer.fit_transform(data[number_var])\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Outliers Check After Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, groups, missing_threshold=5):\n",
    "    results = {'columns_with_outliers': [], 'outlier_counts': {}, 'bounds': {}}\n",
    "\n",
    "    groups = {\n",
    "        'Cuisines': cuisine_columns,\n",
    "        'Days of the Week': dow_columns,\n",
    "        'Hours of the Day': hr_columns,\n",
    "        'Other Features': ['customer_age', 'vendor_count', 'product_count', 'is_chain', 'first_order', 'last_order']\n",
    "    }\n",
    "\n",
    "    for group_name, columns in groups.items():\n",
    "        num_cols = len(columns)\n",
    "        cols_per_row = 4  # Number of columns per row in the grid\n",
    "        rows = -(-num_cols // cols_per_row)  # Ceiling division for number of rows\n",
    "        fig, axes = plt.subplots(rows, cols_per_row, figsize=(16, rows * 4))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, column in enumerate(columns):\n",
    "            if column in df.select_dtypes(include=[np.number]).columns:\n",
    "                Q1 = df[column].quantile(0.25)\n",
    "                Q3 = df[column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                outlier_data = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "                outlier_percent = len(outlier_data) / len(df) * 100\n",
    "\n",
    "                results['bounds'][column] = {'lower_bound': lower_bound, 'upper_bound': upper_bound}\n",
    "                results['outlier_counts'][column] = len(outlier_data)\n",
    "                if outlier_percent > missing_threshold:\n",
    "                    results['columns_with_outliers'].append(column)\n",
    "\n",
    "                sns.boxplot(data=df, x=column, color='orange', ax=axes[i], showfliers=False)\n",
    "                sns.stripplot(data=outlier_data, x=column, color='red', jitter=True, ax=axes[i])\n",
    "                axes[i].set_title(f\"{column}\")\n",
    "\n",
    "        for j in range(len(columns), len(axes)):\n",
    "            fig.delaxes(axes[j])  # Remove unused subplots\n",
    "\n",
    "        plt.suptitle(f\"Boxplots with Outliers for {group_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Columns with more than {}% outliers:\".format(missing_threshold))\n",
    "    print(results['columns_with_outliers'])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_results = detect_outliers_iqr(data, groups, missing_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[['CUI_American', 'CUI_Asian']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We wont remove these outliers, since they are values we consider reasonable even if the mean of the features are close to zero, we know that the mean is influenced by the large amount of zeros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create New Features\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['order_activity_duration'] = data['last_order'] - data['first_order']\n",
    "number_var.append(\"order_activity_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['order_frequency'] = data['product_count'] / (data['order_activity_duration'].replace(0, 1))\n",
    "number_var.append(\"order_frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['loyal_customer'] = data['vendor_count'] < data['product_count']\n",
    "category_var.append(\"loyal_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cuisine_diversity'] = (data[[col for col in number_var if col.startswith('CUI_')]] > 0).sum(axis=1)\n",
    "number_var.append(\"cuisine_diversity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['favorite_cuisine'] = data[[col for col in number_var if col.startswith('CUI_')]].idxmax(axis=1)\n",
    "category_var.append(\"favorite_cuisine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frequent_order_flag'] = data['product_count'] > data['product_count'].mean()\n",
    "category_var.append(\"frequent_order_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekend_spending'] = data['DOW_0'] + data['DOW_6']\n",
    "number_var.append(\"weekend_spending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['week_spending'] = data[['DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5']].sum(axis=1)\n",
    "number_var.append(\"week_spending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['supper_spending'] = data[['HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5']].sum(axis=1)\n",
    "data['breakfast_spending'] = data[['HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10']].sum(axis=1)\n",
    "data['lunch_spending'] = data[['HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15']].sum(axis=1)\n",
    "data['snack_spending'] = data[['HR_16', 'HR_17', 'HR_18']].sum(axis=1)\n",
    "data['dinner_spending'] = data[['HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23']].sum(axis=1)\n",
    "number_var.append(\"supper_spending\")\n",
    "number_var.append(\"breakfast_spending\")\n",
    "number_var.append(\"lunch_spending\")\n",
    "number_var.append(\"snack_spending\")\n",
    "number_var.append(\"dinner_spending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_cuisines = ['CUI_Asian', 'CUI_Chinese', 'CUI_Indian', 'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_Thai']\n",
    "western_cuisines = ['CUI_American', 'CUI_Italian', 'CUI_Street Food / Snacks', 'CUI_Cafe']\n",
    "others_cuisines = ['CUI_Healthy', 'CUI_Beverages', 'CUI_OTHER', 'CUI_Chicken Dishes', 'CUI_Desserts']\n",
    "\n",
    "data['Asian_Cuisines'] = data[asian_cuisines].sum(axis=1)\n",
    "data['Western_Cuisines'] = data[western_cuisines].sum(axis=1)\n",
    "data['Others_Cuisines'] = data[others_cuisines].sum(axis=1)\n",
    "\n",
    "number_var.append(\"Asian_Cuisines\")\n",
    "number_var.append(\"Western_Cuisines\")\n",
    "number_var.append(\"Others_Cuisines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_Cuisine_Orders'] = data['Asian_Cuisines'] + data['Western_Cuisines'] + data['Others_Cuisines']\n",
    "data['Asian_Cuisines_Ratio'] = data['Asian_Cuisines'] / data['Total_Cuisine_Orders']\n",
    "data['Western_Cuisines_Ratio'] = data['Western_Cuisines'] / data['Total_Cuisine_Orders']\n",
    "data['Others_Cuisines_Ratio'] = data['Others_Cuisines'] / data['Total_Cuisine_Orders']\n",
    "\n",
    "number_var.append(\"Total_Cuisine_Orders\")\n",
    "number_var.append(\"Asian_Cuisines_Ratio\")\n",
    "number_var.append(\"Western_Cuisines_Ratio\")\n",
    "number_var.append(\"Others_Cuisines_Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5',\n",
    "       'DOW_6', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7',\n",
    "       'HR_8', 'HR_9', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15',\n",
    "       'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23','CUI_Asian', 'CUI_Chinese', 'CUI_Indian', 'CUI_Japanese', 'CUI_Noodle Dishes',\n",
    "         'CUI_Thai','CUI_American', 'CUI_Italian', 'CUI_Street Food / Snacks', 'CUI_Cafe', 'CUI_Healthy', 'Asian_Cuisines', 'Western_Cuisines', 'Others_Cuisines', 'CUI_Beverages', 'CUI_OTHER', 'CUI_Chicken Dishes', 'CUI_Desserts'] \n",
    "\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "for column in columns_to_remove:\n",
    "    number_var.remove(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Visualizations of New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Spending\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['total_spending'], bins=30, kde=True, color='blue')\n",
    "plt.title(\"Distribution of Total Spending\")\n",
    "plt.xlabel(\"Total Spending\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Average Spending Per Cuisine\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['avg_spending_per_cuisine'], bins=30, kde=True, color='green')\n",
    "plt.title(\"Distribution of Average Spending Per Cuisine\")\n",
    "plt.xlabel(\"Average Spending Per Cuisine\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Order Activity Duration\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['order_activity_duration'], bins=30, kde=True, color='orange')\n",
    "plt.title(\"Distribution of Order Activity Duration\")\n",
    "plt.xlabel(\"Order Activity Duration (Days)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Order Frequency\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['order_frequency'], bins=30, kde=True, color='purple')\n",
    "plt.title(\"Distribution of Order Frequency\")\n",
    "plt.xlabel(\"Order Frequency\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# High Spender Flag\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='high_spender', data=data, palette='pastel')\n",
    "plt.title(\"High Spenders Distribution\")\n",
    "plt.xlabel(\"High Spender\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Loyal Customer Flag\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='loyal_customer', data=data, palette='coolwarm')\n",
    "plt.title(\"Loyal Customers Distribution\")\n",
    "plt.xlabel(\"Loyal Customer\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Cuisine Diversity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['cuisine_diversity'], bins=15, kde=False, color='purple')\n",
    "plt.title(\"Distribution of Cuisine Diversity\")\n",
    "plt.xlabel(\"Number of Unique Cuisines Ordered\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Favorite Cuisine\n",
    "plt.figure(figsize=(12, 6))\n",
    "data['favorite_cuisine'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Favorite Cuisine Distribution\")\n",
    "plt.xlabel(\"Cuisine\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Peak Order Hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='peak_order_hour', data=data, palette='viridis')\n",
    "plt.title(\"Peak Order Hour Distribution\")\n",
    "plt.xlabel(\"Hour of the Day\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.show()\n",
    "\n",
    "# Peak Order Day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='peak_order_day', data=data, palette='coolwarm')\n",
    "plt.title(\"Peak Order Day Distribution\")\n",
    "plt.xlabel(\"Day of the Week (0=Sunday, 6=Saturday)\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.show()\n",
    "\n",
    "# Frequent Order Flag\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='frequent_order_flag', data=data, palette='pastel')\n",
    "plt.title(\"Frequent Order Flag Distribution\")\n",
    "plt.xlabel(\"Frequent Order Flag\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Inactive Days\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['inactive_days'], bins=30, kde=True, color='pink')\n",
    "plt.title(\"Distribution of Inactive Days\")\n",
    "plt.xlabel(\"Inactive Days\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Impute Missing Values of New Features (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data[number_var] = knn_imputer.fit_transform(data[number_var])\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Selection\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Correlation w/ New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data[number_var].corr()\n",
    "\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) >= 0.8 or abs(correlation_matrix.iloc[i, j]) < -0.8: \n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Variable 1', 'Variable 2', 'Correlation'])\n",
    "\n",
    "high_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Cramer V (Categorical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data has all these columns\n",
    "assert all(col in data.columns for col in category_var), \"Some columns are missing in data\"\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    contingency_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(contingency_matrix)[0]\n",
    "    n = contingency_matrix.sum().sum()\n",
    "    r, k = contingency_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "correlation_matrix = pd.DataFrame(index=category_var, columns=category_var)\n",
    "\n",
    "for col1 in category_var:\n",
    "    for col2 in category_var:\n",
    "        if col1 == col2:\n",
    "            correlation_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            correlation_matrix.loc[col1, col2] = cramers_v(\n",
    "                data[col1].round(3),\n",
    "                data[col2].round(3)\n",
    "            )\n",
    "\n",
    "correlation_matrix = correlation_matrix.astype(float)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(correlation_matrix,annot=True, fmt=\".2f\",cmap=\"coolwarm\",vmin=0, vmax=1,cbar=True,)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop vendor_count based on the correlation analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['vendor_count'])\n",
    "number_var.remove('vendor_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Variance Evaluation\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=0.01)\n",
    "data_num = selector.fit_transform(data[number_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = data[number_var].columns[selector.get_support()]\n",
    "data_num = pd.DataFrame(data_num, columns=filtered_columns, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[category_var] = data[category_var].apply(lambda col: col.map(col.value_counts(normalize=True)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=category_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scaling Numerical Data (MinMaxScaler)\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_num = scaler.fit_transform(data_num)\n",
    "data_num = pd.DataFrame(data_num, columns=filtered_columns, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.to_excel(\"Numeric_DM2425_ABCDEats_DATASET.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
