{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1a322e-c44e-4b44-a0ed-18f3061e6066",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis <a class=\"anchor\"><a id='PVA'></a></b><br>\n",
    "Authors:<br><br>\n",
    "Student Name - Gonçalo Custódio<br>\n",
    "- Student id - 20211643<br>\n",
    "- Contact e-mail - 20211643@novaims.unl.pt<br>\n",
    "  \n",
    "Student Name - Diogo Correia<br>\n",
    "- Student id - 20211586<br>\n",
    "- Contact e-mail - 20211586@novaims.unl.pt<br>\n",
    "  \n",
    "Student Name - João Santos<br>\n",
    "- Student id - 20211691<br>\n",
    "- Contact e-mail - 20211691@novaims.unl.pt<br>\n",
    "  \n",
    "Student Name - Nuno Bernardino<br>\n",
    "- Student id - 20211546<br>\n",
    "- Contact e-mail - 20211546@novaims.unl.pt<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14336445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec99db7-4f23-4e61-8b36-84eec278a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sqlite3\n",
    "#import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f8593-9e39-4d20-92c4-615a03d63175",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"DM2425_ABCDEats_DATASET.xlsx\", sheet_name=\"DM2425_ABCDEats_DATASET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68d152",
   "metadata": {},
   "source": [
    "# 1. Deep Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07bc63",
   "metadata": {},
   "source": [
    "**Initial Analysis**\n",
    "\n",
    "To kick off our deep exploration, we’ll use the `data.info()` command to get an overview of the dataset. This command provides essential information, including the number of entries, column names, non-null counts, and data types for each variable. This quick summary will allow us to identify any missing values, spot potential data type issues, and gain a high-level understanding of the dataset's structure, setting the stage for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360ddd1-fc32-415d-a2f8-e282c406cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ba6e0",
   "metadata": {},
   "source": [
    "**To visualize and get a sense of the data**\n",
    "\n",
    "To begin familiarizing ourselves with the dataset, we’ll use the `data.head()` command, which displays the first few rows of the data. This preview allows us to quickly see the contents, spot-check variable values, and gain an initial sense of the data's structure and content. By viewing these rows, we can start to assess the data format, variable types, and any immediate patterns or issues that stand out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6070ee",
   "metadata": {},
   "source": [
    "**Check data types of our variables**\n",
    "\n",
    "In this step, we’ll use the `data.dtypes` command to examine the data types of each variable in our dataset. This overview will confirm if the variables are appropriately typed (e.g., integers, floats, objects) and will help us spot any inconsistencies or unexpected types that might require adjustment. Understanding the data types at this stage is crucial, as it guides us in selecting suitable preprocessing and analysis techniques for each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd047f35",
   "metadata": {},
   "source": [
    "Since we don't agree with the fact that the variables in question (last_promo, payment_method and customer_region) are of type object we will change them to category in order to facilitate future analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5870df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['last_promo'] = data['last_promo'].astype('category')\n",
    "data['payment_method'] = data['payment_method'].astype('category')\n",
    "data['customer_region'] = data['customer_region'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc9f20",
   "metadata": {},
   "source": [
    "We will divide the variables into lists for categorical and numerical variables to facilitate future interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_var = ['customer_region', 'last_promo', 'payment_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_var = ['customer_age', 'vendor_count', 'product_count', 'is_chain', 'first_order', 'last_order', \n",
    "              'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe', 'CUI_Chicken Dishes', 'CUI_Chinese', \n",
    "              'CUI_Desserts', 'CUI_Healthy', 'CUI_Indian', 'CUI_Italian', 'CUI_Japanese', 'CUI_Noodle Dishes', \n",
    "              'CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai', 'DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', \n",
    "              'DOW_5', 'DOW_6', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', \n",
    "              'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', \n",
    "              'HR_21', 'HR_22', 'HR_23']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd84e7",
   "metadata": {},
   "source": [
    "# 2. Summary of Important Statistics\n",
    "\n",
    "In this section, we’ll start by generating a statistical summary using the `data.describe()` command. This summary provides key descriptive statistics for each numerical variable, such as the mean, standard deviation, minimum, and maximum values. By reviewing these statistics, we can understand the central tendencies, dispersion, and overall range of our data. \n",
    "\n",
    "Following this, we’ll delve into more detailed analyses, including frequency distributions for categorical variables, box plots, and histograms, each accompanied by summary boxes with key metrics. Together, these statistics and visualizations give us a comprehensive view of the data, setting a foundation for more advanced analyses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fba97",
   "metadata": {},
   "source": [
    "Sendo assim podemos verificar que algumas variáveis tem outliers\n",
    "\n",
    "Para isso iremos fazer uma função para identificar quantos outliers temos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_summary(df, columns):\n",
    "    outliers_data = []\n",
    "\n",
    "    numeric_columns = df[columns].select_dtypes(include=['number']).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        Q1 = df[col].quantile(0.25) \n",
    "        Q3 = df[col].quantile(0.75)  \n",
    "        IQR = Q3 - Q1 \n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outlier_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
    "        \n",
    "        outliers_data.append({'Coluna': col, 'Número de Outliers': outlier_count})\n",
    "    \n",
    "    outliers_summary = pd.DataFrame(outliers_data)\n",
    "    return outliers_summary\n",
    "\n",
    "outliers_summary_table = detect_outliers_summary(data, number_var)\n",
    "print(outliers_summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cfb054",
   "metadata": {},
   "source": [
    "Como podemos verificar existem bastantes outliers, coisa que mais para a frente iremos tratar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348a2c1",
   "metadata": {},
   "source": [
    "For numerical variables, the statistics above are very effective, but for categorical variables we must count the frequency of each value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf693045",
   "metadata": {},
   "source": [
    "**Frequency of each value in Categorical Variables**\n",
    "\n",
    "To gain insights into the distribution of categorical variables, we will compute the frequency of each unique value in these columns. By understanding the frequency of categories, we can identify dominant categories, assess data balance, and detect potential issues such as sparse categories or outliers that may affect our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052189fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in category_var:\n",
    "    frequency_percentage = data[column].value_counts(normalize=True) * 100\n",
    "    result = pd.DataFrame({'Value': frequency_percentage.index, 'Percentage': frequency_percentage.values})\n",
    "\n",
    "    print(f\"\\033[1mColumn '{column}'\\033[0m:\")\n",
    "    print(result)\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237aa77c",
   "metadata": {},
   "source": [
    "**Now for a better visualization of the data and their respective statistics, let's plot some graphs:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd5a46",
   "metadata": {},
   "source": [
    "**2.1 Box-Plot**\n",
    "\n",
    "Box plots allow us to visualize the spread, central tendency, and range of values for each variable. For each variable, we will generate a box plot that includes a summary box indicating the average value, median, minimum, and maximum values. This helps us identify any skewness, outliers, or unusual patterns in the data, which are essential for informed data preprocessing and decision-making in our data mining process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963bd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "numeric_columns = [col for col in data.select_dtypes(include=['int64', 'float64']).columns if data[col].notna().any()]\n",
    "\n",
    "n_cols = 3\n",
    "\n",
    "n_rows = math.ceil(len(numeric_columns) / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    sns.boxplot(data=data, y=col, ax=axes[i])\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Value\")\n",
    "\n",
    "    mean_value = data[col].mean()\n",
    "    median_value = data[col].median()\n",
    "    min_value = data[col].min()\n",
    "    max_value = data[col].max()\n",
    "\n",
    "    stats_text = (f'Average: {mean_value:.2f}\\n'\n",
    "                  f'Median: {median_value:.2f}\\n'\n",
    "                  f'Min: {min_value:.2f}\\n'\n",
    "                  f'Max: {max_value:.2f}')\n",
    "    \n",
    "    axes[i].text(1.05, 0.5, stats_text, transform=axes[i].transAxes, \n",
    "                  fontsize=12, verticalalignment='center', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fca39",
   "metadata": {},
   "source": [
    "**2.2 Histrogram**\n",
    "\n",
    "Histograms provide a visual representation of the distribution of values for each variable. For each variable, we will create a histogram along with a summary box that displays the average value, median, minimum, and maximum values. These histograms help us understand the frequency distribution and shape (e.g., normal, skewed) of each variable, offering insights that will guide us in selecting appropriate analytical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "numeric_columns = [col for col in data.select_dtypes(include=['int64', 'float64']).columns if data[col].notna().any()]\n",
    "\n",
    "n_cols = 3\n",
    "\n",
    "n_rows = math.ceil(len(numeric_columns) / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 6))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    sns.histplot(data[col], bins=30, ax=axes[i], stat='count') \n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(\"Value\")\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "    mean_value = data[col].mean()\n",
    "    median_value = data[col].median()\n",
    "    min_value = data[col].min()\n",
    "    max_value = data[col].max()\n",
    "\n",
    "    stats_text = (f'Average: {mean_value:.2f}\\n'\n",
    "                  f'Median: {median_value:.2f}\\n'\n",
    "                  f'Min: {min_value:.2f}\\n'\n",
    "                  f'Max: {max_value:.2f}')\n",
    "    \n",
    "    axes[i].text(1.05, 0.5, stats_text, transform=axes[i].transAxes, \n",
    "                  fontsize=12, verticalalignment='center', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced0890",
   "metadata": {},
   "source": [
    "# 3. Identify Trends, Patterns, or Anomalies\n",
    "In this chapter, our objective is to uncover underlying trends, patterns, and anomalies within the data. By systematically analyzing these elements, we can detect recurring behaviors, identify correlations, and highlight unusual values or outliers that might impact our analysis. Understanding these factors is essential in a data mining project, as they often reveal deeper insights and influence the accuracy of predictive models. This analysis will inform our feature engineering choices and guide us toward meaningful data transformations, ultimately enhancing the value and interpretability of our results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1228160",
   "metadata": {},
   "source": [
    "### 3.1 Anomalies\n",
    "\n",
    "In this section, we focus on detecting anomalies within our dataset. Anomalies, or outliers, are values that deviate significantly from the majority of observations and may indicate data entry errors, rare events, or unique patterns. Identifying these values early on is crucial, as they can impact model performance and lead to misleading insights if not handled properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b87b29",
   "metadata": {},
   "source": [
    "When we open the dataset, we notice that there are missing values and hyphen values, because there are blank values and others with only “-”. We need to correct this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc50af2-fffe-49af-81eb-1b38696f83f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "hyphen_counts = (data == '-').sum()\n",
    "\n",
    "total_rows = data.shape[0]\n",
    "missing_percentage = (missing_values / total_rows) * 100\n",
    "hyphen_percentage = (hyphen_counts / total_rows) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Hyphen Values': hyphen_counts,\n",
    "    'Missing %': missing_percentage,\n",
    "    'Hyphen %': hyphen_percentage\n",
    "})\n",
    "\n",
    "missing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar as linhas onde first_order é ausente e last_order é diferente de 0\n",
    "missing_first_order = data[data['first_order'].isnull() & (data['last_order'] != 0)]\n",
    "\n",
    "# Verificar se existe alguma linha com essa condição e exibir o resultado\n",
    "if not missing_first_order.empty:\n",
    "    print(\"Linhas onde 'first_order' é missing e 'last_order' é diferente de 0:\")\n",
    "    print(missing_first_order)\n",
    "else:\n",
    "    print(\"Não existem linhas onde 'first_order' é missing e 'last_order' é diferente de 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"HR_0\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c20f7",
   "metadata": {},
   "source": [
    "Futuramente iremos tratar dos missing values e dos hyphen values\n",
    "\n",
    "*Relativamente à coluna customer_region, entendemos que os hyphen values presentes podem representar valores desconhecidos, por isso, iremos trocar os hyphen values para \"Missing\"*\n",
    "\n",
    "*Relativamente à coluna customer_age, entendemos que os missing values presentes representam que a idade não foi informada pelo cliente, por isso, iremos utilizar uma decision tree para dar input a valores ausentes*\n",
    "\n",
    "*Relativamente à coluna first_order, entendemos que os missing values presentes representam 0 pois sempre que isso acontece, o last_order também é 0*\n",
    "\n",
    "*Relativamente à coluna last_promo, verificamos que existe mais de 50% dos valores com hyphen, o que nos deixa a pensar se dropamos a coluna ou não. Caso não o façamos, iremos trocar esses hyphen values por \"No promo\"*\n",
    "\n",
    "*Relativamente à coluna HR_0, entendemos que como existem 0 values e NaN values, os NaN significam falta de informação, e como os 0 values não são relevantes para a nossa análise, dropamos a coluna em questão*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['HR_0'], inplace=True)\n",
    "\n",
    "if 'HR_0' in number_var:\n",
    "    number_var.remove('HR_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f60981",
   "metadata": {},
   "source": [
    "We need to verify if there are several lines with the same customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = data.columns[0] \n",
    "duplicates = data[data.duplicated(subset=[first_column], keep=False)] \n",
    "\n",
    "print(f\"Repeats values in '{first_column}' column:\")\n",
    "print(duplicates[[first_column]].drop_duplicates()) \n",
    "print(f\"\\nTotal duplicates in '{first_column}' are {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89032d7b",
   "metadata": {},
   "source": [
    "We will check whether the repeated rows with the same id have exactly the same values in the other variables or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f909bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = [\"742ca068fc\",\"b55012ee1c\",\"24251eb7da\",\"671bf0c738\",\"df91183978\",\"6bbf5f74cd\",\"8aa9bbc147\",\"cf563a0a98\",\"201a13a34d\",\"06018a56be\",\"fac7984c0d\",\"b8e7a643a4\",\"cc08ef25ce\"]\n",
    "\n",
    "matching_rows = data[data[data.columns[0]].isin(target_id)]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  \n",
    "pd.set_option(\"display.width\", None) \n",
    "\n",
    "matching_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c6e4d",
   "metadata": {},
   "source": [
    "We can see that the 26 lines with the repeated id are exactly the same in all the variables. So let's get rid of these repeated lines and keep just one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=[first_column], keep='first')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5651176c",
   "metadata": {},
   "source": [
    "### 3.2 Correlation\n",
    "\n",
    "In this section, we examine the correlations between variables to identify relationships and dependencies within the dataset. By analyzing these correlations, we can detect variables that move together, uncover potential redundancies, and gain insights into which variables may have the most predictive power. This step is essential for refining our feature selection and improving model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e052e3",
   "metadata": {},
   "source": [
    "**Correlation table between Numeric Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d67903",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data[number_var].corr()\n",
    "\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) >= 0.5 or abs(correlation_matrix.iloc[i, j]) < -0.5: \n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Variable 1', 'Variable 2', 'Correlation'])\n",
    "\n",
    "high_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6075748",
   "metadata": {},
   "source": [
    "**Heatmaps**\n",
    "\n",
    "Since there are quite a few numerical variables, we'll divide them into groups so that the heatmap isn't overloaded with unreadable information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6014c7e",
   "metadata": {},
   "source": [
    "**Heatmap of Demographic variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995650dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_vars = ['customer_age', 'vendor_count', 'product_count', 'is_chain']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix[demographic_vars].loc[demographic_vars], annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c43b5",
   "metadata": {},
   "source": [
    "**Heatmap of CUI variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb420b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cui_vars = [\n",
    "  #  'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe', 'CUI_Chicken Dishes', \n",
    "  #  'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy', 'CUI_Indian', 'CUI_Italian', \n",
    " #   'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai'\n",
    "#]\n",
    "\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.heatmap(correlation_matrix[cui_vars].loc[cui_vars], annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8dd1f",
   "metadata": {},
   "source": [
    "**Heatmap of DOW variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02972266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dow_vars = ['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6']\n",
    "\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.heatmap(correlation_matrix[dow_vars].loc[dow_vars], annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac87357",
   "metadata": {},
   "source": [
    "**Heatmap of HR variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e334c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_vars = [\n",
    "    'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', \n",
    "    'HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10', 'HR_11', \n",
    "    'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', \n",
    "    'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix[hr_vars].loc[hr_vars], annot=True, fmt=\".2f\", cmap='coolwarm', \n",
    "            mask=correlation_matrix[hr_vars].loc[hr_vars] < 0.1, square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177a8aa",
   "metadata": {},
   "source": [
    "**Heatmap of CUI Foods and the Days of the Week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b145eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_columns = [\n",
    "    'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe', 'CUI_Chicken Dishes', \n",
    "    'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy', 'CUI_Indian', 'CUI_Italian', \n",
    "    'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai'\n",
    "]\n",
    "\n",
    "day_columns = ['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6']\n",
    "\n",
    "selected_columns = food_columns + day_columns\n",
    "correlation_matrix = data[selected_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', \n",
    "            mask=correlation_matrix < 0.1, square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c2d5c",
   "metadata": {},
   "source": [
    "**Correlation between categorical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb652b",
   "metadata": {},
   "source": [
    "**Chi-Squared**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_test(var1, var2):\n",
    "    contingency_table = pd.crosstab(data[var1], data[var2])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    return chi2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1be51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\n",
    "    (\"customer_region\", \"last_promo\"),\n",
    "    (\"customer_region\", \"payment_method\"),\n",
    "    (\"last_promo\", \"payment_method\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for var1, var2 in combinations:\n",
    "    chi2_value, p_value = chi2_test(var1, var2)\n",
    "    results.append({\"Variable 1\": var1, \"Variable 2\": var2, \"Chi-Squared\": chi2_value, \"p-value\": p_value})\n",
    "\n",
    "resultss = pd.DataFrame(results)\n",
    "resultss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fbc51",
   "metadata": {},
   "source": [
    "*It is important to note that the p-value should be low, as there is sufficient evidence to reject the null hypothesis!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0edef",
   "metadata": {},
   "source": [
    "# 4. Create New Features\n",
    "\n",
    "In this chapter, we focus on creating new features to enhance the dataset’s predictive power and capture more meaningful patterns. By engineering features based on existing variables, we can reveal hidden relationships, improve model performance, and better address our project objectives. This process is a key step in preparing our data for analysis, enabling us to extract deeper insights from the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfbcbb",
   "metadata": {},
   "source": [
    "We decided to make for now 5 new features:\n",
    "\n",
    "Age Category: This segmentation can be valuable in tailoring promotions and communications for specific age demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826059cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Category\n",
    "def age_group(age):\n",
    "    if age < 18:\n",
    "        return \"Young\"\n",
    "    elif 18 < age < 65:\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "data[\"age_category\"] = data[\"customer_age\"].apply(age_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a003804",
   "metadata": {},
   "source": [
    "Order Frequency per Customer: This feature can help ABCDEats understand the engagement level of each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order Frequency Per Customer\n",
    "def calculate_order_frequency(df):\n",
    "    return df.groupby('customer_id')['order_id'].transform('count')\n",
    "\n",
    "data['order_frequency_per_customer'] = calculate_order_frequency(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ec60b",
   "metadata": {},
   "source": [
    "Average Order Value: This metric allows segmentation of customers based on spending patterns, helping to identify high-value customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Order Value\n",
    "def calculate_average_order_value(df):\n",
    "    return df.groupby('customer_id')['product_count'].transform('mean')\n",
    "\n",
    "data['average_order_value'] = calculate_average_order_value(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e50868",
   "metadata": {},
   "source": [
    "High-engagement Customer Flag: This binary flag identifies customers whose order frequency or average order value is above the overall average. Customers with a high engagement flag could be potential candidates for loyalty programs or targeted retention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ce810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-Engagement Customer Flag\n",
    "def high_engagement_flag(df):\n",
    "    avg_order_frequency = df['order_frequency_per_customer'].mean()\n",
    "    avg_order_value = df['average_order_value'].mean()\n",
    "    return ((df['order_frequency_per_customer'] > avg_order_frequency) | \n",
    "            (df['average_order_value'] > avg_order_value)).astype(int)\n",
    "\n",
    "data['high_engagement_customer'] = high_engagement_flag(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322eb2a",
   "metadata": {},
   "source": [
    "Não sei o que meter aqui malta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time between first purchase and last purchase\n",
    "data[\"days_between_first_last_purchase\"] = data[\"last_order\"] - data[\"first_order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
